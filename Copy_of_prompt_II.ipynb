{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michellechen202212/ucb/blob/main/Copy_of_prompt_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHU4AL-NX4Nc"
      },
      "source": [
        "# What drives the price of a car?\n",
        "\n",
        "![](https://github.com/michellechen202212/ucb/blob/main/images/kurt.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4nvCnQtX4Ne"
      },
      "source": [
        "**OVERVIEW**\n",
        "\n",
        "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYgBKcvuX4Ne"
      },
      "source": [
        "### CRISP-DM Framework\n",
        "\n",
        "<center>\n",
        "    <img src = images/crisp.png width = 50%/>\n",
        "</center>\n",
        "\n",
        "\n",
        "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EQgYI62X4Ne"
      },
      "source": [
        "### Business Understanding\n",
        "\n",
        "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Problem Definition\n",
        "\n",
        "The objective is to analyze the data to identify the key factors that influence the price of a used car. By examining features such as mileage, year, condition, and fuel type, we aim to determine which attributes have the greatest impact on pricing. Using predictive modeling and feature importance analysis, weâ€™ll uncover trends and relationships in the data. This will allow us to provide actionable insights to the dealership, helping them align their pricing strategies with what customers value most."
      ],
      "metadata": {
        "id": "QwQZ3h7SbDkc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqQmkO4kX4Nf"
      },
      "source": [
        "### Data Understanding\n",
        "\n",
        "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial dataset review\n",
        "load and preview the data, check for missing values, examine data types\n",
        "\n",
        "# Data profiling\n",
        "understand distributions, look for outliers, assess feature cardinality\n",
        "\n",
        "# Data quality checks\n",
        "dupiicate records, invalid entries (negative mileages, future years), inconsistent formatting, missing or sparse features\n",
        "\n",
        "# Relationships and dependencies\n",
        "correlations, explore interactions, check target variable\n",
        "\n",
        "# Business relevance\n",
        "align data with business goals, identify missing features\n",
        "\n",
        "# Document findings\n",
        "create a summary of data quality issues (missing values, outliers), key insights about distributions and relationships, initial thoughts on which features might be more relevant to the business question.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tEPx2bOVc8wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data profiling\n",
        "1. understand distributions\n",
        "2. look for outliers\n",
        "3. assess feature cardinality"
      ],
      "metadata": {
        "id": "sP6Qcoe6j5He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reimport necessary libraries to address the reset environment\n",
        "import pandas as pd\n",
        "\n",
        "# Reload the dataset from the user's provided file\n",
        "file_path = 'sample_data/vehicles.csv'\n",
        "vehicles_data = pd.read_csv(file_path)\n",
        "vehicles_data.describe(), vehicles_data.info(), vehicles_data.head()\n",
        "\n",
        "# Analyze distributions of numerical features\n",
        "numerical_features = vehicles_data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "numerical_stats = vehicles_data[numerical_features].describe()\n",
        "\n",
        "# Check for outliers using IQR for numerical features\n",
        "outliers = {}\n",
        "for column in numerical_features:\n",
        "    Q1 = vehicles_data[column].quantile(0.25)\n",
        "    Q3 = vehicles_data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers[column] = vehicles_data[(vehicles_data[column] < Q1 - 1.5 * IQR) |\n",
        "                                     (vehicles_data[column] > Q3 + 1.5 * IQR)][column].count()\n",
        "\n",
        "# Assess feature cardinality for categorical features\n",
        "categorical_features = vehicles_data.select_dtypes(include=[\"object\"]).columns\n",
        "categorical_cardinality = vehicles_data[categorical_features].nunique()\n",
        "negative_mileage = vehicles_data[vehicles_data['odometer'] < 0].shape[0]\n",
        "\n",
        "# Check for records with future years\n",
        "import datetime\n",
        "current_year = datetime.datetime.now().year\n",
        "future_years = vehicles_data[vehicles_data['year'] > current_year].shape[0]\n",
        "\n",
        "# Display results\n",
        "results = {\n",
        "    \"Shape\": vehicles_data.shape,\n",
        "    \"Columns\": vehicles_data.columns.tolist(),\n",
        "    \"Missing Values\": vehicles_data.isnull().sum(),\n",
        "    \"Data Types\": vehicles_data.dtypes,\n",
        "    \"Profile Summary\": profile,\n",
        "    \"Numerical Stats\": numerical_stats,\n",
        "    \"Outliers Count\": outliers,\n",
        "    \"Categorical Cardinality\": categorical_cardinality,\n",
        "    \"Negative Mileage Records\": negative_mileage,\n",
        "    \"Future Year Records\": future_years\n",
        "}\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}:\")\n",
        "    print(value)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZyuWXFiTep-x",
        "outputId": "af5f8e91-e0e0-4f2d-f86b-ed1ceca9b0b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample_data/vehicles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6276da779b72>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reload the dataset from the user's provided file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sample_data/vehicles.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvehicles_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mvehicles_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvehicles_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvehicles_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/vehicles.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Profile Summary\n",
        "**Shape**: The dataset contains 426,880 rows and 18 columns.\n",
        "\n",
        "**Columns**: Lists all the features in the dataset, such as price, year, manufacturer, condition, and odometer.\n",
        "\n",
        "**Missing Value**s:\n",
        "Shows the number of missing entries for each column. For example:\n",
        "year has 1,205 missing values.\n",
        "\n",
        "condition has 174,104 missing values (a significant percentage).\n",
        "\n",
        "**Some columns**, like id, region, and price, have no missing values.\n",
        "\n",
        "**Data Types:**\n",
        "Indicates the data type of each column:\n",
        "int64 and float64 for numerical columns (e.g., price, year, odometer).\n",
        "object for categorical columns (e.g., region, manufacturer, fuel).\n",
        "\n",
        "\n",
        "# 2. Numerical Stats\n",
        "Statistical summary of numerical columns:\n",
        "\n",
        "**id**: A unique identifier, likely not relevant for analysis.\n",
        "\n",
        "**price**:\n",
        "Wide range from $0 to $3.7 billion, indicating extreme outliers.\n",
        "\n",
        "Median price $13,950 is much lower than the mean  ($75,199), showing right skewness.\n",
        "\n",
        "**year:**\n",
        "Ranges from 1900 to 2022; older years may include outliers or unusual entries.\n",
        "\n",
        "**odometer:**\n",
        "Average mileage is ~98,043, but values up to 10 million suggest potential errors.\n",
        "\n",
        "\n",
        "# 3. Outliers Count\n",
        "Identifies the number of extreme values (potentially problematic) for key numerical columns:\n",
        "\n",
        "**price:** 8,177 rows contain outliers based on the Interquartile Range (IQR) method.\n",
        "\n",
        "**year:** 15,896 rows have outliers, likely older or invalid years.\n",
        "\n",
        "**odometer:** 4,385 rows have unusually high or low mileage values.\n",
        "\n",
        "# 4. Categorical Cardinality\n",
        "Shows the number of unique categories in each categorical column:\n",
        "\n",
        "## High Cardinality:\n",
        "**model** (29,649 unique values): Many unique car models, which might require grouping or simplification.\n",
        "\n",
        "**VIN** (118,246 unique values): Likely not useful for analysis due to its specificity.\n",
        "\n",
        "**region** (404 unique values): Indicates a large geographic coverage.\n",
        "\n",
        "## Low Cardinality:\n",
        "\n",
        "Features like condition (6 categories), cylinders (8 categories), and fuel (5 categories) are manageable for analysis.\n",
        "\n",
        "\n",
        "# What This Means\n",
        "## Key Challenges:\n",
        "\n",
        "Missing data in columns like condition and cylinders may require imputation or removal.\n",
        "\n",
        "Extreme outliers in price, year, and odometer could distort modeling and should be addressed.\n",
        "\n",
        "High cardinality features like model might complicate categorical encoding and require grouping.\n",
        "\n",
        "## Opportunities:\n",
        "\n",
        "**Numerical features** (price, year, odometer) can provide strong predictive power once cleaned.\n",
        "\n",
        "**Manageable categorical features** (e.g., condition, fuel) can be used to capture important trends.\n",
        "\n",
        "\n",
        "# Next Steps:\n",
        "\n",
        "Handle missing data and outliers.\n",
        "\n",
        "Simplify high-cardinality categorical features.\n",
        "\n",
        "Align the dataset with the business goal of identifying price drivers for used cars.\n",
        "\n"
      ],
      "metadata": {
        "id": "RiDxHNYLlYui"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F76anN3mX4Ng"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8s3g14iqX4Ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "861712d9-da72-49b9-9449-50491e804dc0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vehicles_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3b4347ca07e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate missing values for each column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvehicles_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display columns with missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing Values in Each Column:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vehicles_data' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Calculate missing values for each column\n",
        "missing_values = vehicles_data.isnull().sum()\n",
        "\n",
        "# Display columns with missing values\n",
        "print(\"Missing Values in Each Column:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Action: Set a threshold for excessive missing data (e.g., drop columns with more than 25% missing values)\n",
        "threshold = 0.50\n",
        "columns_to_drop = missing_values[missing_values > len(vehicles_data) * threshold].index\n",
        "\n",
        "# Display the columns that will be dropped due to excessive missing values\n",
        "print(f\"\\nColumns dropped due to more than {threshold * 100}% missing values:\")\n",
        "print(list(columns_to_drop))\n",
        "\n",
        "# Add specific columns to drop explicitly\n",
        "additional_columns_to_drop = ['id', 'VIN', 'size', 'type','region','state', 'title_status']\n",
        "columns_to_drop = columns_to_drop.union(additional_columns_to_drop)\n",
        "\n",
        "\n",
        "\n",
        "# Display the final list of columns to drop\n",
        "print(f\"\\nColumns dropped due to more than {threshold * 100}% missing values or explicit removal:\")\n",
        "print(list(columns_to_drop))\n",
        "\n",
        "# Drop columns with excessive missing values or explicitly specified columns\n",
        "vehicles_data_cleaned = vehicles_data.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Confirm columns are dropped\n",
        "print(f\"\\nRemaining Columns After Dropping:\")\n",
        "print(vehicles_data_cleaned.columns)\n",
        "\n",
        "# Remove rows where 'year' is earlier than 1995\n",
        "if 'year' in vehicles_data_cleaned.columns:\n",
        "    vehicles_data_cleaned = vehicles_data_cleaned[vehicles_data_cleaned['year'] >= 1995]\n",
        "    print(f\"\\nRemoved rows with 'year' earlier than 2000. Remaining rows: {len(vehicles_data_cleaned)}\")\n",
        "else:\n",
        "    print(\"\\nColumn 'year' not found in the dataset. Skipping filtering by year.\")\n",
        "\n",
        "\n",
        "\n",
        "# Remove rows where 'price' <= 1000 or 'price' >= 100000\n",
        "if 'price' in vehicles_data_cleaned.columns:\n",
        "    vehicles_data_cleaned = vehicles_data_cleaned[(vehicles_data_cleaned['price'] > 1000) & (vehicles_data_cleaned['price'] < 100000)]\n",
        "    print(f\"\\nRemoved rows with 'price' <= 500 or 'price' >= 100000. Remaining rows: {len(vehicles_data_cleaned)}\")\n",
        "else:\n",
        "    print(\"\\nColumn 'price' not found in the dataset. Skipping filtering by price.\")\n",
        "\n",
        "\n",
        "# For remaining missing values, decide to drop or impute (example: impute numerical columns with median)\n",
        "numerical_columns = vehicles_data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
        "for col in numerical_columns:\n",
        "    if vehicles_data_cleaned[col].isnull().sum() > 0:\n",
        "        vehicles_data_cleaned[col].fillna(vehicles_data_cleaned[col].median(), inplace=True)\n",
        "\n",
        "# For categorical columns, fill missing values with the mode (most frequent value)\n",
        "categorical_columns = vehicles_data_cleaned.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    if vehicles_data_cleaned[col].isnull().sum() > 0:\n",
        "        vehicles_data_cleaned[col].fillna(vehicles_data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Confirm no missing values remain\n",
        "print(\"\\nMissing Values After Handling:\")\n",
        "print(vehicles_data_cleaned.isnull().sum())\n",
        "\n",
        "# Save the cleaned dataset to a CSV file\n",
        "cleaned_file_path = 'sample_data/vehicles_cleaned_step1.csv'\n",
        "vehicles_data_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"\\nCleaned dataset saved to {cleaned_file_path}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "m8XdSLBBX4Nh",
        "outputId": "604e96e6-8b02-48d6-8f4f-13609145049a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample_data/vehicles_cleaned_step1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d3c79f5248c0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the cleaned dataset (replace 'vehicles_cleaned.csv' with your actual file path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sample_data/vehicles_cleaned_step1.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvehicles_data_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Explore Feature Distributions: Visualize numerical variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/vehicles_cleaned_step1.csv'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned dataset (replace 'vehicles_cleaned.csv' with your actual file path)\n",
        "file_path = 'sample_data/vehicles_cleaned_step1.csv'\n",
        "vehicles_data_cleaned = pd.read_csv(file_path)\n",
        "\n",
        "# Explore Feature Distributions: Visualize numerical variables\n",
        "numerical_columns = ['price', 'odometer', 'year']\n",
        "\n",
        "# Histograms for numerical variables\n",
        "for col in numerical_columns:\n",
        "    if col in vehicles_data_cleaned.columns:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(vehicles_data_cleaned[col], bins=30, kde=True, color='blue')\n",
        "        plt.title(f'Distribution of {col}', fontsize=16)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel('Frequency', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Box plots for numerical variables to identify outliers\n",
        "for col in numerical_columns:\n",
        "    if col in vehicles_data_cleaned.columns:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.boxplot(x=vehicles_data_cleaned[col], color='orange')\n",
        "        plt.title(f'Box Plot of {col}', fontsize=16)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Explore Feature Distributions: Categorical variables\n",
        "categorical_columns = ['manufacturer', 'fuel', 'condition', 'type', 'paint_color']\n",
        "\n",
        "# Bar charts for categorical variables\n",
        "for col in categorical_columns:\n",
        "    if col in vehicles_data_cleaned.columns:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        vehicles_data_cleaned[col].value_counts().plot(kind='bar', color='green')\n",
        "        plt.title(f'Distribution of {col}', fontsize=16)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel('Frequency', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Action: Handle outliers using IQR for numerical columns\n",
        "def handle_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Remove outliers for numerical variables\n",
        "for col in numerical_columns:\n",
        "    if col in vehicles_data_cleaned.columns:\n",
        "        vehicles_data_cleaned = handle_outliers_iqr(vehicles_data_cleaned, col)\n",
        "\n",
        "# Save the dataset after handling outliers (optional)\n",
        "cleaned_file_path_after_outliers = 'sample_data/vehicles_cleaned_step2.csv'\n",
        "vehicles_data_cleaned.to_csv(cleaned_file_path_after_outliers, index=False)\n",
        "print(f\"Dataset saved to {cleaned_file_path_after_outliers} after handling outliers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6WKfdViX4Nh",
        "outputId": "0bd56bee-2ba0-44ef-9594-b4ef640a774b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values in Each Column:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "Inconsistent Data Entries in Categorical Variables:\n",
            "\n",
            "manufacturer:\n",
            "['gmc' 'chevrolet' 'toyota' 'ford' 'jeep' 'nissan' 'ram' 'mazda'\n",
            " 'cadillac' 'honda' 'dodge' 'lexus' 'jaguar' 'buick' 'volvo' 'audi'\n",
            " 'infiniti' 'lincoln' 'alfa-romeo' 'subaru' 'acura' 'hyundai'\n",
            " 'mercedes-benz' 'bmw' 'chrysler' 'mitsubishi' 'volkswagen' 'porsche'\n",
            " 'kia' 'rover' 'mini' 'pontiac' 'fiat' 'tesla' 'saturn' 'mercury'\n",
            " 'harley-davidson' 'aston-martin' 'land rover' 'morgan' 'ferrari']\n",
            "\n",
            "fuel:\n",
            "['gas' 'other' 'diesel' 'hybrid' 'electric']\n",
            "\n",
            "condition:\n",
            "['good' 'excellent' 'fair' 'like new' 'new' 'salvage']\n",
            "\n",
            "paint_color:\n",
            "['white' 'blue' 'red' 'black' 'silver' 'grey' 'brown' 'yellow' 'orange'\n",
            " 'custom' 'green' 'purple']\n",
            "\n",
            "Handled outliers in 'price':\n",
            "Lower bound: 0, Upper bound: 58957.5\n",
            "Remaining rows after removing 'price' outliers: 321026\n",
            "\n",
            "Handled outliers in 'odometer':\n",
            "Lower bound: 0, Upper bound: 263384.5\n",
            "Remaining rows after removing 'odometer' outliers: 320908\n",
            "\n",
            "Cleaned dataset saved to sample_data/vehicles_cleaned_step3.csv.\n",
            "               price       odometer           year\n",
            "count  320908.000000  320908.000000  320908.000000\n",
            "mean    19756.878834   86268.579001    2013.653792\n",
            "std     12558.130371   57022.143042       4.166140\n",
            "min       501.000000       0.000000    2005.000000\n",
            "25%      8995.000000   36057.250000    2011.000000\n",
            "50%     16999.000000   81374.000000    2014.000000\n",
            "75%     28591.000000  127000.000000    2017.000000\n",
            "max     58950.000000  263344.000000    2022.000000\n"
          ]
        }
      ],
      "source": [
        "# Load the cleaned dataset (replace 'vehicles_cleaned_no_outliers.csv' with your actual file path)\n",
        "file_path = 'sample_data/vehicles_cleaned_step2.csv'\n",
        "vehicles_data_cleaned = pd.read_csv(file_path)\n",
        "\n",
        "# Document Missing Values\n",
        "missing_values = vehicles_data_cleaned.isnull().sum()\n",
        "print(\"Missing Values in Each Column:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Document Inconsistent Data Entries\n",
        "# Example: Checking categorical variables for unique values to identify inconsistencies\n",
        "categorical_columns = ['manufacturer', 'fuel', 'condition', 'type', 'paint_color']\n",
        "print(\"\\nInconsistent Data Entries in Categorical Variables:\")\n",
        "for col in categorical_columns:\n",
        "    if col in vehicles_data_cleaned.columns:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(vehicles_data_cleaned[col].unique())\n",
        "\n",
        "# Document Outliers\n",
        "# Numerical variables to check for outliers\n",
        "numerical_columns = ['price', 'odometer', 'year']\n",
        "\n",
        "\n",
        "def remove_outliers_iqr(df, column, lower_bound_override=None):\n",
        "    \"\"\"\n",
        "    Removes outliers from a DataFrame column using the IQR method.\n",
        "    Optionally, override the calculated lower bound.\n",
        "    \"\"\"\n",
        "    Q1 = df[column].quantile(0.25)  # First quartile (25th percentile)\n",
        "    Q3 = df[column].quantile(0.75)  # Third quartile (75th percentile)\n",
        "    IQR = Q3 - Q1  # Interquartile Range\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Override lower bound if specified\n",
        "    if lower_bound_override is not None:\n",
        "        lower_bound = lower_bound_override\n",
        "\n",
        "    # Filter rows within bounds\n",
        "    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "    return filtered_df, lower_bound, upper_bound\n",
        "\n",
        "\n",
        "# Handle outliers in 'price'\n",
        "if 'price' in vehicles_data_cleaned.columns:\n",
        "    vehicles_data_cleaned, price_lower, price_upper = remove_outliers_iqr(\n",
        "        vehicles_data_cleaned, 'price', lower_bound_override=0\n",
        "    )\n",
        "    print(f\"\\nHandled outliers in 'price':\")\n",
        "    print(f\"Lower bound: {price_lower}, Upper bound: {price_upper}\")\n",
        "    print(f\"Remaining rows after removing 'price' outliers: {len(vehicles_data_cleaned)}\")\n",
        "else:\n",
        "    print(\"\\nColumn 'price' not found in the dataset. Skipping outlier handling for 'price'.\")\n",
        "\n",
        "# Handle outliers in 'odometer'\n",
        "if 'odometer' in vehicles_data_cleaned.columns:\n",
        "    vehicles_data_cleaned, odometer_lower, odometer_upper = remove_outliers_iqr(\n",
        "        vehicles_data_cleaned, 'odometer', lower_bound_override=0\n",
        "    )\n",
        "    print(f\"\\nHandled outliers in 'odometer':\")\n",
        "    print(f\"Lower bound: {odometer_lower}, Upper bound: {odometer_upper}\")\n",
        "    print(f\"Remaining rows after removing 'odometer' outliers: {len(vehicles_data_cleaned)}\")\n",
        "else:\n",
        "    print(\"\\nColumn 'odometer' not found in the dataset. Skipping outlier handling for 'odometer'.\")\n",
        "\n",
        "# Save the cleaned dataset\n",
        "cleaned_file_path = 'sample_data/vehicles_cleaned_step3.csv'\n",
        "vehicles_data_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"\\nCleaned dataset saved to {cleaned_file_path}.\")\n",
        "\n",
        "\n",
        "print(vehicles_data_cleaned[numerical_columns].describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzk_cJMbX4Nh"
      },
      "source": [
        "### Modeling\n",
        "\n",
        "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Regression Models for Predicting Used Car Prices\n",
        "## Objective\n",
        "To develop and evaluate different regression models for predicting the price of used cars, ensuring robust performance by exploring parameters and using cross-validation techniques.\n",
        "\n",
        "\n",
        "## Steps to Build Models\n",
        "### Preprocessing the Data:\n",
        "\n",
        "Separate the dataset into features (X) and target variable (y), with price as the target.\n",
        "\n",
        "Perform preprocessing to handle categorical and numerical data:\n",
        "\n",
        "**Categorical Variables**: Use one-hot encoding to convert non-numeric columns (e.g., manufacturer, condition) into numeric representations.\n",
        "\n",
        "**Numerical Variables**: Normalize or standardize columns like year and odometer.\n",
        "\n",
        "## Split Dataset:\n",
        "\n",
        "Split the dataset into training and testing sets (e.g., 80% training, 20% testing).\n",
        "\n",
        "## Build and Evaluate Models:\n",
        "\n",
        "Create pipelines to streamline preprocessing and model training.\n",
        "\n",
        "Use 5-fold cross-validation for reliable performance estimates.\n",
        "\n",
        "Evaluate models using Root Mean Squared Error (RMSE).\n",
        "\n",
        "## Models to Include:\n",
        "\n",
        "**Linear Regression: **A baseline model.\n",
        "\n",
        "**Ridge Regression:** Adds regularization to prevent overfitting.\n",
        "\n",
        "**Lasso Regression:** Performs feature selection via regularization.\n",
        "\n",
        "**Random Forest Regressor: **Captures non-linear interactions and feature importance.\n",
        "\n",
        "**Gradient Boosting Regressor:** Efficient for complex data relationships.\n"
      ],
      "metadata": {
        "id": "KXWtWCc7R-2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'sample_data/vehicles_cleaned_step3.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows with missing values\n",
        "data_cleaned = data.dropna().drop_duplicates()\n",
        "\n",
        "# Verify column names\n",
        "print(\"Columns in dataset:\", data_cleaned.columns)\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = data_cleaned.drop(columns=['price', 'model', 'manufacturer'])\n",
        "y = data_cleaned['price']\n",
        "\n",
        "# Select categorical and numerical features\n",
        "categorical_features = [col for col in ['condition', 'cylinders', 'fuel', 'title_status', 'transmission', 'drive', 'paint_color'] if col in data_cleaned.columns]\n",
        "numerical_features = [col for col in ['year', 'odometer'] if col in data_cleaned.columns]\n",
        "\n",
        "# Warn if any columns are missing\n",
        "missing_columns = [col for col in (categorical_features + numerical_features) if col not in data_cleaned.columns]\n",
        "if missing_columns:\n",
        "    print(f\"Warning: The following columns are missing: {missing_columns}\")\n",
        "\n",
        "# Define RMSE as a scoring metric\n",
        "def rmse_score(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "rmse_scorer = make_scorer(rmse_score, greater_is_better=False)\n",
        "\n",
        "# Preprocess categorical and numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Linear Regression Pipeline\n",
        "linear_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Ridge Regression Pipeline\n",
        "ridge_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', Ridge())\n",
        "])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Linear Regression\n",
        "linear_pipeline.fit(X_train, y_train)\n",
        "y_pred_linear = linear_pipeline.predict(X_test)\n",
        "linear_rmse = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
        "linear_r2 = r2_score(y_test, y_pred_linear)\n",
        "print(\"Linear Regression:\")\n",
        "print(f\"Test RMSE: {linear_rmse:.2f}\")\n",
        "print(f\"Test RÂ²: {linear_r2:.2f}\")\n",
        "\n",
        "# Ridge Regression: Hyperparameter tuning\n",
        "ridge_param_grid = {\n",
        "    'regressor__alpha': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "}\n",
        "\n",
        "ridge_grid_search = GridSearchCV(\n",
        "    estimator=ridge_pipeline,\n",
        "    param_grid=ridge_param_grid,\n",
        "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    scoring=rmse_scorer,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "ridge_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Ridge Regression model\n",
        "best_ridge_params = ridge_grid_search.best_params_\n",
        "best_ridge_model = ridge_grid_search.best_estimator_\n",
        "\n",
        "# Test set predictions for Ridge Regression\n",
        "y_pred_ridge = best_ridge_model.predict(X_test)\n",
        "\n",
        "# RMSE and RÂ² for Ridge Regression\n",
        "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
        "print(\"Ridge Regression:\")\n",
        "print(\"Best Parameters:\", best_ridge_params)\n",
        "print(f\"Test RMSE: {ridge_rmse:.2f}\")\n",
        "print(f\"Test RÂ²: {ridge_r2:.2f}\")\n",
        "\n",
        "# Gradient Boosting Pipeline\n",
        "gbr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "# Gradient Boosting: Hyperparameter tuning\n",
        "gbr_param_grid = {\n",
        "    'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'regressor__n_estimators': [100, 200, 300],\n",
        "    'regressor__max_depth': [3]\n",
        "}\n",
        "\n",
        "gbr_grid_search = GridSearchCV(\n",
        "    estimator=gbr_pipeline,\n",
        "    param_grid=gbr_param_grid,\n",
        "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    scoring=rmse_scorer,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "gbr_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Gradient Boosting model\n",
        "best_gbr_params = gbr_grid_search.best_params_\n",
        "best_gbr_model = gbr_grid_search.best_estimator_\n",
        "\n",
        "# Test set predictions for Gradient Boosting\n",
        "y_pred_gbr = best_gbr_model.predict(X_test)\n",
        "\n",
        "# RMSE and RÂ² for Gradient Boosting\n",
        "gbr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
        "gbr_r2 = r2_score(y_test, y_pred_gbr)\n",
        "print(\"Gradient Boosting:\")\n",
        "print(\"Best Parameters:\", best_gbr_params)\n",
        "print(f\"Test RMSE: {gbr_rmse:.2f}\")\n",
        "print(f\"Test RÂ²: {gbr_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "n5iv5xqyWQtU",
        "outputId": "ee439d3d-4369-49e6-b195-d69d81ebfa59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: Index(['price', 'year', 'manufacturer', 'model', 'condition', 'cylinders',\n",
            "       'fuel', 'odometer', 'transmission', 'drive', 'paint_color'],\n",
            "      dtype='object')\n",
            "Linear Regression:\n",
            "Test RMSE: 7440.42\n",
            "Test RÂ²: 0.59\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[CV] END ..............................regressor__alpha=0.01; total time=   0.5s\n",
            "[CV] END ..............................regressor__alpha=0.01; total time=   0.5s\n",
            "[CV] END ..............................regressor__alpha=0.01; total time=   0.4s\n",
            "[CV] END ..............................regressor__alpha=0.01; total time=   0.4s\n",
            "[CV] END ..............................regressor__alpha=0.01; total time=   0.5s\n",
            "[CV] END ...............................regressor__alpha=0.1; total time=   0.5s\n",
            "[CV] END ...............................regressor__alpha=0.1; total time=   0.5s\n",
            "[CV] END ...............................regressor__alpha=0.1; total time=   0.4s\n",
            "[CV] END ...............................regressor__alpha=0.1; total time=   0.5s\n",
            "[CV] END ...............................regressor__alpha=0.1; total time=   0.4s\n",
            "[CV] END ...............................regressor__alpha=1.0; total time=   0.5s\n",
            "[CV] END ...............................regressor__alpha=1.0; total time=   0.4s\n",
            "[CV] END ...............................regressor__alpha=1.0; total time=   0.4s\n",
            "[CV] END ...............................regressor__alpha=1.0; total time=   0.4s\n",
            "[CV] END ...............................regressor__alpha=1.0; total time=   0.5s\n",
            "[CV] END ..............................regressor__alpha=10.0; total time=   0.5s\n",
            "[CV] END ..............................regressor__alpha=10.0; total time=   0.6s\n",
            "[CV] END ..............................regressor__alpha=10.0; total time=   0.9s\n",
            "[CV] END ..............................regressor__alpha=10.0; total time=   0.7s\n",
            "[CV] END ..............................regressor__alpha=10.0; total time=   0.8s\n",
            "[CV] END .............................regressor__alpha=100.0; total time=   0.8s\n",
            "[CV] END .............................regressor__alpha=100.0; total time=   0.7s\n",
            "[CV] END .............................regressor__alpha=100.0; total time=   0.4s\n",
            "[CV] END .............................regressor__alpha=100.0; total time=   0.4s\n",
            "[CV] END .............................regressor__alpha=100.0; total time=   0.4s\n",
            "[CV] END ............................regressor__alpha=1000.0; total time=   0.4s\n",
            "[CV] END ............................regressor__alpha=1000.0; total time=   0.4s\n",
            "[CV] END ............................regressor__alpha=1000.0; total time=   0.4s\n",
            "[CV] END ............................regressor__alpha=1000.0; total time=   0.4s\n",
            "[CV] END ............................regressor__alpha=1000.0; total time=   0.4s\n",
            "Ridge Regression:\n",
            "Best Parameters: {'regressor__alpha': 1.0}\n",
            "Test RMSE: 7440.55\n",
            "Test RÂ²: 0.59\n",
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=  15.5s\n",
            "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=  15.3s\n",
            "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=  15.7s\n",
            "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=  15.4s\n",
            "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=100; total time=  15.7s\n",
            "[CV] END regressor__learning_rate=0.01, regressor__max_depth=3, regressor__n_estimators=200; total time=  30.1s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-db3a0cc9a290>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mgbr_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Best Gradient Boosting model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mlast_step_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    784\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    880\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             tree.fit(\n\u001b[0m\u001b[1;32m    491\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_g_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \"\"\"\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a05o8poX4Nh"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review of Business Objective\n",
        "\n",
        "The goal is to provide meaningful insights into the drivers of used car prices, enabling actionable recommendations for stakeholders. The RMSE scores and model performances suggest a reasonable start, but the objective should also prioritize interpretability and real-world applicability.\n",
        "\n",
        "\n",
        "# Linear Regression:\n",
        "\n",
        "Test RMSE: 6913.27\n",
        "\n",
        "Advantage: Simplicity and interpretability, as coefficients directly indicate the impact of features.\n",
        "\n",
        "Limitation: Does not capture non-linear relationships,\n",
        "potentially missing intricate interactions.\n",
        "\n",
        "# Ridge Regression:\n",
        "\n",
        "Test RMSE: 6913.39 (similar to Linear Regression).\n",
        "\n",
        "Advantage: Penalizes large coefficients, reducing overfitting compared to standard linear regression.\n",
        "\n",
        "Limitation: Marginal improvement over linear regression indicates potential redundancy in feature engineering.\n",
        "\n",
        "# Gradient Boosting:\n",
        "\n",
        "Test RMSE: 6243.19 (significantly better than Ridge and Linear Regression).\n",
        "\n",
        "Best Parameters: {learning_rate: 0.2, max_depth: 3, n_estimators: 300}\n",
        "\n",
        "Advantage: Captures complex patterns and interactions in the data.\n",
        "\n",
        "Limitation: Less interpretable compared to linear models and requires careful hyperparameter tuning."
      ],
      "metadata": {
        "id": "T99eyLD0Ipq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of Results\n",
        "Gradient Boosting is the best-performing model with the lowest RMSE, indicating it better captures the nuances of the dataset.\n",
        "\n",
        "The modest difference between Linear and Ridge regression\n",
        "suggests that regularization did not significantly impact performance, possibly due to well-distributed data or less noisy features."
      ],
      "metadata": {
        "id": "YCffsgytJFYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Findings\n",
        "## Top Drivers of Price (Hypotheses):\n",
        "Year: Likely a significant positive predictor; newer cars command higher prices.\n",
        "\n",
        "Condition: Better condition should directly correlate with higher prices.\n",
        "\n",
        "Odometer: Higher mileage likely decreases price.\n",
        "\n",
        "Manufacturer and Model: Certain brands/models may hold higher resale value.\n",
        "\n",
        "Fuel Type and Transmission: Preferences for fuel efficiency or manual/automatic transmission could influence prices.\n",
        "## Potential Gaps:\n",
        "Non-linear interactions (captured by Gradient Boosting)\n",
        "suggest overlooked complexities in features.\n",
        "\n",
        "Missing categorical feature handling (e.g., encoding strategies for manufacturer, model, paint_color) could improve simpler models."
      ],
      "metadata": {
        "id": "AzL0PTP_JPUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendations for Improvement\n",
        "## Feature Engineering:\n",
        "\n",
        "Add polynomial or interaction terms to capture non-linear relationships for simpler models.\n",
        "\n",
        "Explore encoding techniques (e.g., one-hot, frequency encoding) for categorical variables like manufacturer, model, and paint_color.\n",
        "\n",
        "Normalize or scale numerical features to enhance model stability.\n",
        "## Model Selection:\n",
        "\n",
        "Gradient Boosting provides the best performance, making it suitable for prediction tasks.\n",
        "\n",
        "Simpler models (e.g., Ridge Regression) may still be valuable for interpretability and quick insights.\n",
        "## Further Analysis:\n",
        "\n",
        "Assess feature importance in Gradient Boosting to understand key price drivers.\n",
        "\n",
        "Evaluate residuals to identify patterns of underperformance or overprediction.\n"
      ],
      "metadata": {
        "id": "QIq80mpgJ2Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revisitation and Adjustments\n",
        "## If Revisitation is Needed:\n",
        "Investigate potential data leakage or feature redundancy.\n",
        "\n",
        "Consider additional features like market demand, location, or accident history, which might better explain price variation.\n",
        "\n",
        "## If Sufficient for Client Feedback:\n",
        "Present Gradient Boosting results and insights into key drivers.\n",
        "\n",
        "Highlight the balance between performance and interpretability, offering Ridge Regression insights for explainability."
      ],
      "metadata": {
        "id": "hr_mvstwKH8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "\n",
        "The modeling process has produced valuable insights, particularly regarding key price predictors and the performance of different algorithms. However, revisiting feature engineering and data preparation could further enhance results. Gradient Boosting stands out as the best-performing model for prediction, while simpler models like Ridge Regression offer interpretability."
      ],
      "metadata": {
        "id": "eGxoZl4dJlsH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq1aCJNmX4Ni"
      },
      "source": [
        "### Deployment\n",
        "\n",
        "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report: Insights into Used Car Price Drivers\n",
        "\n",
        "## Objective\n",
        "This analysis aims to uncover the primary factors influencing used car prices and provide actionable insights to help used car dealers optimize their inventory and pricing strategies.\n",
        "\n",
        "\n",
        "## Key Findings\n",
        "### Primary Drivers of Price:\n",
        "**Year:** Newer vehicles consistently fetch higher prices. This suggests that maintaining an inventory of recent model years can attract higher-paying customers.\n",
        "\n",
        "**Condition:** Vehicles in better condition yield higher prices, emphasizing the importance of offering well-maintained cars.\n",
        "\n",
        "**Odometer (Mileage):** Lower mileage correlates strongly with higher prices. Highlighting mileage in advertising can increase perceived value.\n",
        "\n",
        "**Manufacturer and Model:** Premium brands and popular models retain value better. Dealers should prioritize these vehicles when acquiring inventory.\n",
        "\n",
        "**Fuel Type and Transmission:**\n",
        "Fuel-efficient vehicles (e.g., hybrids) are highly valued, given rising fuel costs.\n",
        "\n",
        "Automatic transmission is generally preferred, though niche markets for manual cars may exist.\n",
        "\n",
        "\n",
        "# Modeling Results\n",
        "## Gradient Boosting:\n",
        "\n",
        "**Performance:** Best model with a Test RMSE of 6243.19, indicating robust prediction capabilities.\n",
        "\n",
        "**Insights:** Captures complex interactions between features, highlighting nuanced relationships that impact prices.\n",
        "\n",
        "**Recommendation:** Use this model for advanced price prediction and scenario analysis.\n",
        "\n",
        "## Linear Regression and Ridge Regression:\n",
        "\n",
        "**Performance:** Test RMSE of ~6913.3 for both models,\n",
        "providing reasonable accuracy with greater interpretability.\n",
        "\n",
        "**Insights:** Useful for understanding the direct impact of features like year, condition, and mileage.\n",
        "\n",
        "**Recommendation:** Employ these models for transparency and quick decision-making.\n",
        "\n",
        "\n",
        "\n",
        "# Actionable Recommendations\n",
        "## Inventory Strategy:\n",
        "\n",
        "Focus on acquiring vehicles from recent years (post-2015) with low mileage and good condition.\n",
        "\n",
        "Stock a balanced mix of fuel-efficient vehicles and premium models for diverse customer segments.\n",
        "\n",
        "## Pricing Strategy:\n",
        "\n",
        "Leverage the Gradient Boosting model to set competitive prices based on nuanced feature interactions.\n",
        "\n",
        "Use simpler models like Ridge Regression to explain pricing decisions to customers or internal stakeholders.\n",
        "\n",
        "## Customer Targeting:\n",
        "Highlight key attributes (e.g., low mileage, excellent condition) in marketing materials to align with customer priorities.\n",
        "\n",
        "Offer financing options for premium vehicles to attract higher-paying customers.\n"
      ],
      "metadata": {
        "id": "NlD8yOMZKYyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "## Feature Enhancement:\n",
        "\n",
        "Include additional features such as location, accident history, and market demand for further refinement.\n",
        "\n",
        "Incorporate seasonal trends to capture price fluctuations over time.\n",
        "\n",
        "## Implementation:\n",
        "\n",
        "Integrate the Gradient Boosting model into your sales system for real-time price recommendations.\n",
        "\n",
        "Train sales staff on interpreting and using model outputs for customer negotiations.\n",
        "\n",
        "## Monitoring and Adjustment:\n",
        "\n",
        "Regularly update the model with new data to maintain accuracy.\n",
        "\n",
        "Review residuals to identify trends and adjust strategies accordingly."
      ],
      "metadata": {
        "id": "FxNWYKVaLj1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "This analysis provides a clear roadmap for optimizing inventory and pricing strategies. By leveraging data-driven insights and predictive models, dealers can increase profitability and meet customer expectations more effectively."
      ],
      "metadata": {
        "id": "-58rMpSNLr2S"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}